{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsSc+BL8ZgSt9lu21jGCwJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCanela-1954/DataSci_Course/blob/main/%5BDATA-04%5D%20Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [DATA-04] Embeddings"
      ],
      "metadata": {
        "id": "Ufr69NCUPONt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is an embedding?"
      ],
      "metadata": {
        "id": "n8JMZDycPZjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, an **embedding** is a representation of a piece of information, such as a word, a sentence or an image, as a vector in a space of a given dimension, called the **embedding dimension**.\n",
        "\n",
        "Embedding vectors are one of the basic ingredients in the top performing language models that are all the rage nowadays. Typical **embedding dimensions**, in that context, range from 500 to 4,000. Nevertheless, for many applications, like the example that follows this note, you can use lower dimensions, saving time and memory.\n",
        "\n",
        "For an embedding to be useful, \"similar\" pieces of information must be represented by vectors that are close in a geometric sense (the distance between the endpoints, or the angle). For instance, in a word embedding, words with similar meanings, such as 'nice' and 'beautiful', will be represented by close vectors. Unrelated words, such as 'computer' and 'dog', will be represented by non-close vectors."
      ],
      "metadata": {
        "id": "XiMCv_FeP4Do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applications of embeddings"
      ],
      "metadata": {
        "id": "IUhUrkoWRpma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector embeddings open the door to using classic data mining techniques with difficult data. Since every data unit becomes a point in an Euclidean space, we can create groups by means of a clustering algorithm, or detect outliers. Standard examples are:\n",
        "\n",
        "* **Clustering**. Suppose that, based on purchase records, we have a representation of customers as points in a embedding space. Then, with a standard clustering algorithm, we can create customer segments. Since customers with similar records will be represented as close points, they will be in the same segment. Note that, since the coordinates of these points are \"abstract\" numbers, these segments must be profiled in terms of the products purchased, or in terms of features such as age, gender, etc.\n",
        "\n",
        "* **Classification**. The coordinates of a data unit in the embedding representation can be taken as the *X* variables for a regression or classification model, to predict an outcome such as a price or a purchase decision.\n",
        "\n",
        "* **Recommendation**. This is explained later in this note, and illustrated in the example that follows.\n",
        "\n",
        "* **Outlier detection**. In the embedding space, the outliers will show up as the points that are far from the rest of the \"population\".\n",
        "\n",
        "Other applications of vector embeddings are related to **large language models** (LLM's), and are out of the scope of this course (but not less relevant)."
      ],
      "metadata": {
        "id": "KgqCuDF-RuAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings"
      ],
      "metadata": {
        "id": "rC8ptBTXSPqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text embeddings can be created at different levels: character, word, etc. **Word embeddings** gave a strong push to **language processing** in 2013, when Google released **Word2Vec**. A word embedding consists of a list or **vocabulary**  of words and the corresponding list of vectors, all of the same dimension. Word2Vec was not a single embedding, but a methodology allowing the choice of different options, among them the embedding dimension.\n",
        "\n",
        "In the classic version, the WordVec vocabulary is a collection of words, and the **training data set** is a collection of documents or sentences. The vectors assigned to two words will be similar if the the words are used in a similar way in the training data. But the same approach can be applied to different data. For instance, the vocabulary can be a collection of grocery products and the training data set a collection of shopping lists. The embedding vectors for two products will be similar when one of them can replace the other one in the customers' shopping baskets. This approach can be used in recommendation, as we will do in the example."
      ],
      "metadata": {
        "id": "w8-GStJfVANy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity measures\n"
      ],
      "metadata": {
        "id": "zKKK7Yx_VGB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make an embedding representation operational, a mathematical formula for the **similarity** has to be specified. Two typical similarity measures are those based on the distance and the angle. A math refresher, just in case you need it.\n",
        "\n",
        "First, the **distance**. Take two vectors $x$ and $y$ in a space of dimension $n$. Imagine them as two arrows whose origin is the zero point. Then, the distance between the endpoints is given by the formula (Pythagoras theorem):\n",
        "$$\\hbox{dist}(x, y)^2 = \\sum_{i=1}^n(x_i - y_i)^2.$$\n",
        "\n",
        "The **angle** measurement is operationalized through the **cosine**. The cosine formula has been used for years as a similarity measure in data mining, in particular in natural language processing. Mathematically, the cosine works as a correlation, so vectors pointing in the same direction have cosine 1, perpendicular vectors have cosine 0, and vectors pointing in opposite directions have cosine -1.\n",
        "\n",
        "The cosine of the angle determined by two vectors $x$ and $y$ can be calculated as\n",
        "$$\\cos(x, y) = \\frac{\\displaystyle x\\cdot y}{\\lVert x\\rVert\\lVert y\\rVert}.$$\n",
        "\n",
        "In this formula, the numerator is the **dot product** (`dot()` in NumPy, `SUMPRODUCT()` in Excel)\n",
        "$$x\\cdot y = \\sum_{i=1}^n x_i y_i$$\n",
        "and the denominator is the product of the lengths of the vectors (length meaning here the distance from the origin to the endpoint, not the number of terms). This is given by the formula\n",
        "$$\\lVert x\\rVert^2 = \\sum_{i=1}^n x_i^2.$$\n",
        "\n",
        "If the embedding vectors are **normalized**, with length 1, which is the default of some embedding models, the denominator in the cosine formula is not needed, and the cosine is just the output of the function `dot()`. Distances and cosines can be easily calculated in this way. But you can also get them by means of specific functions from several Python packages."
      ],
      "metadata": {
        "id": "j-eLi4quXB06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a recommender system?"
      ],
      "metadata": {
        "id": "-nR1JWNEYL07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **recommender system** seeks to predict the rating or preference that a subject would give to a product (*e.g*. music, books or movies) or social element (*e.g*. people or groups) that he/she/it had not yet considered. These two sides of the system are called the **user** and the **item**, respectively.\n",
        "\n",
        "Recommender systems are common nowadays. Some examples are:\n",
        "\n",
        "* Movie and TV show recommendations (Netflix, Hulu, IMDb).\n",
        "\n",
        "* Product recommendations (Amazon, eBay, Etsy).\n",
        "\n",
        "* Music recommendations (Spotify, Apple Music, Pandora).\n",
        "\n",
        "* News and article recommendations (Google News, Flipboard).\n",
        "\n",
        "* Social network recommendations (Facebook, LinkedIn, Twitter)."
      ],
      "metadata": {
        "id": "WZgLQEG44caO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative filtering"
      ],
      "metadata": {
        "id": "HR1cAW-D4f7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collaborative filtering** (CF) has been used in e-commerce for many years. The recommendation is based on a list of **top-N** recommended products.\n",
        "\n",
        "In e-commerce sites, CF systems recommend products to a target customer based on the opinions of other customers. Opinions can be given explicitly as **ratings**, or implicitly derived from purchase and/or browsing records. In this last case, we talk about **implicit ratings**. In a CF system, nothing has to be known about the customers and products, except ratings, either explicit or implicit.\n",
        "\n",
        "There are two approaches, both based on the **neighbor** concept, which is used in many data mining methods. The concept is easily grasped in specific examples. For instance, in e-commerce sites like Amazon, the neighbors of a customer are those customers who buy the same products that he/she buys. The neighbors of a product are those products bought by the same customers that buy that product. In practice, the neighbors are extracted from the data by means of a similarity measure.\n",
        "\n",
        "* In **user-based** systems, every user has a set of neighbors, called the **neighborhood**. Once a set of neighbors is formed, the system calculates a **score** for every item by averaging the ratings given by the neighbors. The recommendations are based on the scores of the items that the user has not previously considered. The underlying idea is that a customer is likely to do as similar customers do.\n",
        "\n",
        "* In **item-based systems**, the representation is the same, but the neighborhoods are formed with the items. The idea is, now, that a customer is likely to buy products which are similar to those that he/she has already bought. Note that, in this context, the similarity between two items is not based on any characteristic of the items themselves, but on how they are regarded by the users.\n",
        "\n",
        "The similarity measures used in this context are typically variations of the\n",
        "cosine formula, applied to a vector representation of the users/items. Many approaches have proposed for generating this representation, so you can find copious literature on that. Vector embeddings produced by language models are the favorite approach nowadays."
      ],
      "metadata": {
        "id": "zMJvqy5-4jSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content-based recommender systems"
      ],
      "metadata": {
        "id": "2jcvfFQH4uJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roughly speaking, **content-based recommendation** is based on a description of the item characteristics. In this approach, the vector embeddings are extracted from the descriptions.\n",
        "\n",
        "Content-based recommendation has two advantages: (a) it does not require a large user group as CF systems, and (b) new items can be immediately recommended, as soon as their characteristics are available. Even CF systems need this for new products, which nobody has rated/purchased before. This is called the **cold-start problem**."
      ],
      "metadata": {
        "id": "jA30z0QF4-s5"
      }
    }
  ]
}