{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM0b9VGPaDc/KE6bI2zaeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MCanela-1954/DataSci_Course/blob/main/%5BDATA-%2002%5D%20Querying%20data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [DATA-02] Querying data"
      ],
      "metadata": {
        "id": "aRa-5dENqFuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sorting data in Pandas"
      ],
      "metadata": {
        "id": "7i9gfG4wqRex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two **sorting** methods in Pandas. A series can be sorted either by the index or by the values, with the methods `.sort_index()` and `.sort_values()`, respectively. Both methods work for data frames, but, for the second one, you have to specify either the name of a column or a list of column names, which will then be used in the order that you wrote them.\n",
        "\n",
        "The parameter `ascending` allows you to choose between ascending and descending ways. The default is `ascending=True`."
      ],
      "metadata": {
        "id": "PypWwS2iqnR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing values"
      ],
      "metadata": {
        "id": "6pevtL72qrK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing values** are denoted by `NaN` in Pandas. When a Pandas object is built, both Python's `None` and NumPy's `nan` are taken as `NaN`. Since `np.nan` has type `float`, numeric columns containing `NaN` values get type `float`.\n",
        "\n",
        "Three useful Pandas methods related to missing values, which can be applied to both series and data frames, are:\n",
        "\n",
        "* `.isna()` returns a Boolean Pandas object of the same shape, indicating which terms are missing.\n",
        "\n",
        "* `.fillna(value)` fills missing values.\n",
        "\n",
        "* `.dropna()` removes the rows that contain at least one missing value."
      ],
      "metadata": {
        "id": "prO5Sj2vqvIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates"
      ],
      "metadata": {
        "id": "ZDmGO84cqy2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two useful Pandas methods for managing **duplicates**:\n",
        "\n",
        "* `.duplicated()` returns a Boolean series indicating the rows that are duplicated. The default of this method performs a top-down check of the data, returning `False` for the values occurring for the first time, and `True` for those having occurred before. You can reverse this with the argument `keep=last`.\n",
        "\n",
        "* `.drop_duplicates()` drops the duplicated rows. It is based on the Boolean mask created by `.duplicated()`."
      ],
      "metadata": {
        "id": "ZezoyHFgq2ND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping and aggregation"
      ],
      "metadata": {
        "id": "xCiBIk4bq6PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When exploring data, we often use tables for discovering patterns. They can be produced in various ways in Pandas:\n",
        "\n",
        "* The method `.value_counts()` extracts a **frequency table**. The table contains the counts of the occurrences of every value of a given series. It does not include the missing values.\n",
        "\n",
        "* The function `crosstab()` performs a **cross tabulation**. For two series of the same length `s1` and `s2`, the syntax is `pd.crosstab(s1, s2)`. Then, `s1` will be placed on the rows and `s2` on the columns. The default output of `crosstab()` is a frequency table, unless an array of values (parameter `values`) and an **aggregate function** (parameter `aggfunc`) are passed.\n",
        "\n",
        "* The function `pivot_table()` extracts a **spreadsheet-style pivot table** involving the columns of a Pandas data frame. In the simplest version, the syntax is `pd.pivot_table(df, values=col1, index=col2)`. This returns a **one-way table** containing the average value of the column `col1` for the groups defined by the column `col2`. Instead of the average, you can get a choice of summary statistics by means of an argument `aggfunc=[f1, f2, ...]`. With an additional argument `columns=col3`, you get a **two-way table**. For two-way tables, this function works the same as `crosstab()`, but it can only be applied to columns from the same data frame.\n",
        "\n",
        "* The method `.groupby()` groups the rows of a data frame so that an aggregate function can be applied to every group, extracting a **SQL-like table** as a data frame. The syntax is `df.groupby(by=col).f()`. To apply more than one function, use `df.groupby(by=col).agg([f1, f2, ...])`."
      ],
      "metadata": {
        "id": "cEDTclS7q9pQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting data from Pandas"
      ],
      "metadata": {
        "id": "Jpp2nDJfrB9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas provides a toolkit to export/import tabular data sets, which is quite easy to use. Just as the function `read_csv()` and the method `.to_csv()` allow us to switch from Pandas data frames to CSV files and conversely, similar tools exist for JSON and SQL.\n",
        "\n",
        "The method `df.to_json()` writes the data from a Pandas data frame `df` to a text file containing a JSON object. The recommended syntax is\n",
        "\n",
        "```\n",
        "df.to_json('path/fname.json', orient='records')\n",
        "```\n",
        "\n",
        "Since JSON objects can have different structures, combining square and curly braces in different ways, we have to control this with the parameter `orient`. The argument `orient='records'` will create a structure that, in Python, will be seen as a list of dictionaries, each dictionary accounting for a row of the data frame.\n",
        "\n",
        "Data in JSON format can be stored and queried directly in some NoSQL databases, such as **MongoDB**, not as popular as **relational databases** such as **MySQL**. Though relational databases share a language, SQL (not completely standardized), they are different, so you need a specific **driver** to connect to a database from a external application (not only from Python). In Python, those drivers come, as many other things, as packages.\n",
        "\n",
        "A singular case is that of SQLite3, which is a serverless database which comes with any Python distribution. Also, the package `sqlite3` is part of the Python Standard Library.\n",
        "\n",
        "To connect to a database, you must open a **connection** to that database. For SQLite databses, this is quite straightforward:\n",
        "\n",
        "```\n",
        "conn = sqlite3.connect('path/dbname.db')\n",
        "```\n",
        "\n",
        "Once the connection has been established, you can create a table in that database filling it with the data from a data frame. This is done with the method `.to_sql()`. Here, istead of naming the destination file, you name the destination table:\n",
        "\n",
        "```\n",
        "df.to_sql('tablename', conn)\n",
        "```\n",
        "\n",
        "Finally, you close the connection:\n",
        "\n",
        "```\n",
        "conn.close()\n",
        "```\n"
      ],
      "metadata": {
        "id": "pzspqM7VuDIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying a database from Pandas"
      ],
      "metadata": {
        "id": "4IRl6jrLxHio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Pandas, you can run a queries to a database from the Python kernel, once you are connected to tne database. Keeping the notation of the preceding section, the syntax would be:\n",
        "\n",
        "```\n",
        "query_output = pd.read_sql(query, conn)\n",
        "```\n",
        "\n",
        "Here, `query` is a string containing an SQL query. The query output of the query is imported as a data frame."
      ],
      "metadata": {
        "id": "P18kYf_xxU89"
      }
    }
  ]
}
